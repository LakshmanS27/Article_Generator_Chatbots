
# ğŸ“ Multi-LLM Article Generator

This project provides **three separate article generators**, each powered by a different LLM:
- ğŸ¦™ **LLaMA 2 (via GGUF and llama-cpp)**
- ğŸ§  **Phi-3 Mini**
- âœ¨ **Gemini (Google Generative AI)**

Each implementation runs on **Streamlit** with a public link using **ngrok**, and allows you to enter a topic and generate a well-structured, informative article.

---

## ğŸš€ Features

âœ… LLaMA-based article generation using `llama-cpp-python` and GGUF models  
âœ… Phi-3 Mini article generation using `transformers` (GPU-enabled)  
âœ… Gemini article generation using `langchain-google-genai`  
âœ… Runs on Google Colab with GPU/TPU  
âœ… Streamlit UI with `ngrok` public URL  
âœ… Separate helper scripts (`llama_helper.py`, `phi3_helper.py`, `gemini_helper.py`)  

---

## ğŸ“‚ File Structure

```
â”œâ”€â”€ llama_helper.py       # Helper for LLaMA
â”œâ”€â”€ phi3_helper.py        # Helper for Phi-3 Mini
â”œâ”€â”€ gemini_helper.py      # Helper for Gemini API
â”œâ”€â”€ main.py               # Streamlit app (switch as needed)
â”œâ”€â”€ requirements.txt      # Python dependencies
```

---

## ğŸ› ï¸ Installation

Run these commands in **Google Colab**.

### Common Dependencies

```bash
!pip install -q streamlit pyngrok
```

---

### ğŸ¦™ LLaMA Article Generator

ğŸ“¥ Install LLaMA dependencies:
```bash
!pip install -q streamlit pyngrok langchain llama-cpp-python huggingface_hub langchain-community
!pip freeze > requirements.txt
```

ğŸ“„ Download GGUF Model:
```python
from huggingface_hub import hf_hub_download

model_path = hf_hub_download(
    repo_id="TheBloke/Llama-2-7B-Chat-GGUF",
    filename="llama-2-7b-chat.Q4_K_M.gguf"
)
```

ğŸ§ª Helper:
- `llama_helper.py` contains:
  - LangChain LlamaCpp
  - Prompt template & chain

ğŸŒ Run:
```bash
streamlit run main.py
```

---

### ğŸ§  Phi-3 Mini Article Generator

ğŸ“¥ Install Phi-3 dependencies:
```bash
!pip install -q streamlit pyngrok transformers accelerate bitsandbytes langchain huggingface_hub gpt4all
!pip freeze > requirements.txt
```

ğŸ§ª Helper:
- `phi3_helper.py` contains:
  - Transformers with `microsoft/phi-3-mini-4k-instruct`
  - GPU + bfloat16 inference
  - Generates up to 500 tokens

ğŸŒ Run:
```bash
streamlit run main.py
```

---

### âœ¨ Gemini Article Generator

ğŸ“¥ Install Gemini dependencies:
```bash
!pip install -q streamlit pyngrok langchain langchain-community google-generativeai langchain-google-genai
!pip freeze > requirements.txt
```

ğŸ” Gemini API Key:
- You will be prompted to enter your Google Gemini API key when running the helper generation cell.

ğŸ§ª Helper:
- `gemini_helper.py` contains:
  - LangChain ChatGoogleGenerativeAI
  - Prompt template & chain

ğŸŒ Run:
```bash
streamlit run main.py
```

---

## ğŸ“œ Notes

- **LLaMA**
  - Runs locally with `llama-cpp-python`
  - Requires GGUF model (downloaded from HF Hub)
  - Can leverage GPU layers if available

- **Phi-3**
  - Uses `transformers`
  - Runs on GPU (`torch_dtype=bfloat16`)
  - Outputs may be longer and more creative

- **Gemini**
  - Cloud-based (requires API key)
  - Fast and generally most coherent for general topics
  - No GPU required since itâ€™s hosted

---

## âš–ï¸ Evaluation

| Feature                    | Best Choice |
|----------------------------|-------------|
| Quality & Reliability      | âœ¨ Gemini |
| Offline / No Cloud API     | ğŸ¦™ LLaMA |
| GPU-heavy Local Use        | ğŸ§  Phi-3 Mini |
